{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:35:09.694970Z",
     "start_time": "2020-08-19T07:35:04.976977Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.49:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>news_classification</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f909f745f98>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"news_classification\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:35:13.092046Z",
     "start_time": "2020-08-19T07:35:12.948336Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import * #CountVectorizer,StringIndexer, RegexTokenizer,StopWordsRemover\n",
    "from pyspark.sql.functions import * #col, udf,regexp_replace,isnull\n",
    "from pyspark.sql.types import * #StringType,IntegerType\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# For pipeline development\n",
    "from pyspark.ml import Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:35:22.264042Z",
     "start_time": "2020-08-19T07:35:15.617277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>URL</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>STORY</th>\n",
       "      <th>HOSTNAME</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Fed official says weak data caused by weather,...</td>\n",
       "      <td>http://www.latimes.com/business/money/la-fi-mo...</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.latimes.com</td>\n",
       "      <td>1394470370698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
       "      <td>http://www.livemint.com/Politics/H2EvwJSK2VE6O...</td>\n",
       "      <td>Livemint</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.livemint.com</td>\n",
       "      <td>1394470371207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID                                              TITLE  \\\n",
       "0  1  Fed official says weak data caused by weather,...   \n",
       "1  2  Fed's Charles Plosser sees high bar for change...   \n",
       "\n",
       "                                                 URL          PUBLISHER  \\\n",
       "0  http://www.latimes.com/business/money/la-fi-mo...  Los Angeles Times   \n",
       "1  http://www.livemint.com/Politics/H2EvwJSK2VE6O...           Livemint   \n",
       "\n",
       "  CATEGORY                          STORY          HOSTNAME      TIMESTAMP  \n",
       "0        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM   www.latimes.com  1394470370698  \n",
       "1        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM  www.livemint.com  1394470371207  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv(\"./data/uci-news-aggregator.csv\", inferSchema=True, header=True)\n",
    "df.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:35:25.638545Z",
     "start_time": "2020-08-19T07:35:24.652614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422937"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:35:29.864310Z",
     "start_time": "2020-08-19T07:35:29.668101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|               TITLE|CATEGORY|\n",
      "+--------------------+--------+\n",
      "|Fed official says...|       b|\n",
      "|Fed's Charles Plo...|       b|\n",
      "+--------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_category = df.select(\"TITLE\",\"CATEGORY\")\n",
    "news_category.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:35:46.769902Z",
     "start_time": "2020-08-19T07:35:44.608947Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can not infer schema from empty dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6c246b3f0716>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mnull_columns_calc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnull_value_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_category\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnull_columns_calc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Column_Name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Null_Values_Count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Null_Value_Percent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark3/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    600\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[1;32m    601\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark3/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark3/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark3/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \"\"\"\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can not infer schema from empty dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: can not infer schema from empty dataset"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "def null_value_calc(df):\n",
    "    null_columns_counts = []\n",
    "    numRows = df.count()\n",
    "    for k in df.columns:\n",
    "        nullRows = df.where(col(k).isNull()).count()\n",
    "        if(nullRows > 0):\n",
    "            temp = k,nullRows,(nullRows/numRows)*100\n",
    "            null_columns_counts.append(temp)\n",
    "    return(null_columns_counts)\n",
    "\n",
    "null_columns_calc_list = null_value_calc(news_category)\n",
    "spark.createDataFrame(null_columns_calc_list, ['Column_Name', 'Null_Values_Count','Null_Value_Percent']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:36:01.868581Z",
     "start_time": "2020-08-19T07:35:59.538646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|Category            |count |\n",
      "+--------------------+------+\n",
      "|e                   |152127|\n",
      "|b                   |115935|\n",
      "|t                   |108237|\n",
      "|m                   |45616 |\n",
      "|Us Magazine         |31    |\n",
      "|Contactmusic.com    |20    |\n",
      "|GossipCop           |20    |\n",
      "|Complex.com         |12    |\n",
      "|CBS News            |12    |\n",
      "|The Hollywood Gossip|11    |\n",
      "|HipHopDX            |11    |\n",
      "|We Got This Covered |10    |\n",
      "|HeadlinePlanet.com  |10    |\n",
      "|Gamepur             |8     |\n",
      "|Consequence of Sound|7     |\n",
      "|Wetpaint            |7     |\n",
      "|WorstPreviews.com   |7     |\n",
      "|TooFab.com          |7     |\n",
      "|The Escapist        |6     |\n",
      "|Reality TV World    |5     |\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_category.groupBy(\"Category\").count().orderBy(col(\"count\").desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:35:39.631348Z",
     "start_time": "2020-08-19T07:35:39.601747Z"
    }
   },
   "outputs": [],
   "source": [
    "news_category = news_category.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:21:44.055482Z",
     "start_time": "2020-08-19T07:21:43.022812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+--------+\n",
      "|TITLE                                                                   |CATEGORY|\n",
      "+------------------------------------------------------------------------+--------+\n",
      "|Fed official says weak data caused by weather, should not slow taper    |b       |\n",
      "|Fed's Charles Plosser sees high bar for change in pace of tapering      |b       |\n",
      "|US open: Stocks fall after Fed official hints at accelerated tapering   |b       |\n",
      "|Fed risks falling 'behind the curve', Charles Plosser says              |b       |\n",
      "|Fed's Plosser: Nasty Weather Has Curbed Job Growth                      |b       |\n",
      "|Plosser: Fed May Have to Accelerate Tapering Pace                       |b       |\n",
      "|Fed's Plosser: Taper pace may be too slow                               |b       |\n",
      "|Fed's Plosser expects US unemployment to fall to 6.2% by the end of 2014|b       |\n",
      "|US jobs growth last month hit by weather:Fed President Charles Plosser  |b       |\n",
      "|ECB unlikely to end sterilisation of SMP purchases - traders            |b       |\n",
      "+------------------------------------------------------------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_category.show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:40:47.932026Z",
     "start_time": "2020-08-19T07:40:46.295877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Category|count|\n",
      "+--------+-----+\n",
      "|e       |15107|\n",
      "|b       |11584|\n",
      "|m       |11350|\n",
      "|t       |10873|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_cat = news_category.sampleBy(\"CATEGORY\", fractions={'e': 0.1, 'b': 0.1, 't' : 0.1 , 'm' : 0.25}, seed=0)\n",
    "\n",
    "news_cat.groupBy(\"Category\").count().orderBy(col(\"count\").desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:41:08.040624Z",
     "start_time": "2020-08-19T07:41:07.854826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|               TITLE|CATEGORY|\n",
      "+--------------------+--------+\n",
      "|US open: Stocks f...|       b|\n",
      "|ECB unlikely to e...|       b|\n",
      "|Noyer Says Strong...|       b|\n",
      "|Eurozone banks' s...|       b|\n",
      "|Omega's Cooperman...|       b|\n",
      "|EBay asks shareho...|       b|\n",
      "|EBay asks shareho...|       b|\n",
      "|eBay's John Donah...|       b|\n",
      "|McDonald's Report...|       b|\n",
      "|Burn: McDonald's ...|       b|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_cat.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:41:24.944627Z",
     "start_time": "2020-08-19T07:41:24.893053Z"
    }
   },
   "outputs": [],
   "source": [
    "news_cat = news_cat.withColumn(\"TITLE\", regexp_replace(col('TITLE'), '[^A-Za-z ]+', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:41:26.638990Z",
     "start_time": "2020-08-19T07:41:26.599604Z"
    }
   },
   "outputs": [],
   "source": [
    "news_cat = news_cat.withColumn(\"TITLE\", lower(col('TITLE')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:41:31.770989Z",
     "start_time": "2020-08-19T07:41:31.463960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------+--------+--------------------------------------------------------------------------------+\n",
      "|TITLE                                                               |CATEGORY|words                                                                           |\n",
      "+--------------------------------------------------------------------+--------+--------------------------------------------------------------------------------+\n",
      "|us open stocks fall after fed official hints at accelerated tapering|b       |[us, open, stocks, fall, after, fed, official, hints, at, accelerated, tapering]|\n",
      "|ecb unlikely to end sterilisation of smp purchases  traders         |b       |[ecb, unlikely, to, end, sterilisation, of, smp, purchases, traders]            |\n",
      "+--------------------------------------------------------------------+--------+--------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- TITLE: string (nullable = true)\n",
      " |-- CATEGORY: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol=\"TITLE\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "raw_words = regex_tokenizer.transform(news_cat)\n",
    "raw_words.show(2,False)\n",
    "raw_words.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:41:35.906703Z",
     "start_time": "2020-08-19T07:41:35.810830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# Define a list of stop words or use default list\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "stopwords = remover.getStopWords() \n",
    "\n",
    "# Display default list\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:21:45.189515Z",
     "start_time": "2020-08-19T07:21:44.806669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------+--------+--------------------------------------------------------------------------------+---------------------------------------------------------------+\n",
      "|TITLE                                                              |CATEGORY|words                                                                           |filtered                                                       |\n",
      "+-------------------------------------------------------------------+--------+--------------------------------------------------------------------------------+---------------------------------------------------------------+\n",
      "|fed official says weak data caused by weather should not slow taper|b       |[fed, official, says, weak, data, caused, by, weather, should, not, slow, taper]|[fed, official, says, weak, data, caused, weather, slow, taper]|\n",
      "+-------------------------------------------------------------------+--------+--------------------------------------------------------------------------------+---------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_df = remover.transform(raw_words)\n",
    "words_df.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:21:46.829701Z",
     "start_time": "2020-08-19T07:21:45.192118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|               TITLE|CATEGORY|               words|            filtered|label|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|fed official says...|       b|[fed, official, s...|[fed, official, s...|  1.0|\n",
      "|feds charles plos...|       b|[feds, charles, p...|[feds, charles, p...|  1.0|\n",
      "|us open stocks fa...|       b|[us, open, stocks...|[us, open, stocks...|  1.0|\n",
      "|fed risks falling...|       b|[fed, risks, fall...|[fed, risks, fall...|  1.0|\n",
      "|feds plosser nast...|       b|[feds, plosser, n...|[feds, plosser, n...|  1.0|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- TITLE: string (nullable = true)\n",
      " |-- CATEGORY: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- label: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"CATEGORY\", outputCol=\"label\")\n",
    "feature_data = indexer.fit(words_df).transform(words_df)\n",
    "feature_data.show(5)\n",
    "feature_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:41:51.652702Z",
     "start_time": "2020-08-19T07:41:49.874994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------+--------+--------------------------------------------------------------------------------+---------------------------------------------------------------------+-----+\n",
      "|TITLE                                                               |CATEGORY|words                                                                           |filtered                                                             |label|\n",
      "+--------------------------------------------------------------------+--------+--------------------------------------------------------------------------------+---------------------------------------------------------------------+-----+\n",
      "|us open stocks fall after fed official hints at accelerated tapering|b       |[us, open, stocks, fall, after, fed, official, hints, at, accelerated, tapering]|[us, open, stocks, fall, fed, official, hints, accelerated, tapering]|1.0  |\n",
      "+--------------------------------------------------------------------+--------+--------------------------------------------------------------------------------+---------------------------------------------------------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol=\"TITLE\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# raw_words = regex_tokenizer.transform(news_category)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=regex_tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "# words_df = remover.transform(raw_words)\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"CATEGORY\", outputCol=\"label\")\n",
    "# feature_data = indexer.fit(words_df).transform(words_df)\n",
    "\n",
    "pipeline = Pipeline(stages=[regex_tokenizer,remover,indexer])\n",
    "data_prep_pl = pipeline.fit(news_cat)\n",
    "\n",
    "\n",
    "feature_data = data_prep_pl.transform(news_cat)\n",
    "feature_data.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T09:23:04.718620Z",
     "start_time": "2020-08-19T09:23:03.141040Z"
    }
   },
   "outputs": [],
   "source": [
    "# Count Vector (count vectorizer and hashingTF are basically the same thing)\n",
    "# cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\")\n",
    "# model = cv.fit(feature_data)\n",
    "# countVectorizer_features = model.transform(feature_data)\n",
    "\n",
    "# Hashing TF\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawfeatures\", numFeatures=20)\n",
    "HTFfeaturizedData = hashingTF.transform(feature_data)\n",
    "\n",
    "# TF-IDF\n",
    "idf = IDF(inputCol=\"rawfeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(HTFfeaturizedData)\n",
    "TFIDFfeaturizedData = idfModel.transform(HTFfeaturizedData)\n",
    "TFIDFfeaturizedData.name = 'TFIDFfeaturizedData'\n",
    "\n",
    "#rename the HTF features to features to be consistent\n",
    "HTFfeaturizedData = HTFfeaturizedData.withColumnRenamed(\"rawfeatures\",\"features\")\n",
    "HTFfeaturizedData.name = 'HTFfeaturizedData' #We will use later for printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:42:13.736902Z",
     "start_time": "2020-08-19T07:42:05.202846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"filtered\", outputCol=\"features\")\n",
    "model = word2Vec.fit(feature_data)\n",
    "\n",
    "W2VfeaturizedData = model.transform(feature_data)\n",
    "# W2VfeaturizedData.show(1,False)\n",
    "\n",
    "# W2Vec Dataframes typically has negative values so we will correct for that here so that we can use the Naive Bayes classifier\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(W2VfeaturizedData)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaled_data = scalerModel.transform(W2VfeaturizedData)\n",
    "scaled_data\n",
    "W2VfeaturizedData = scaled_data.select('TITLE','CATEGORY','label','scaledFeatures')\n",
    "W2VfeaturizedData = W2VfeaturizedData.withColumnRenamed('scaledFeatures','features')\n",
    "\n",
    "W2VfeaturizedData.name = 'W2VfeaturizedData' # We will need this to print later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:42:20.809013Z",
     "start_time": "2020-08-19T07:42:20.755788Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def ClassTrainEval(classifier,features,classes,train,test):\n",
    "\n",
    "    def FindMtype(classifier):\n",
    "        # Intstantiate Model\n",
    "        M = classifier\n",
    "        # Learn what it is\n",
    "        Mtype = type(M).__name__\n",
    "        \n",
    "        return Mtype\n",
    "    \n",
    "    Mtype = FindMtype(classifier)\n",
    "    \n",
    "\n",
    "    def IntanceFitModel(Mtype,classifier,classes,features,train):\n",
    "        \n",
    "        if Mtype == \"OneVsRest\":\n",
    "            # instantiate the base classifier.\n",
    "            lr = LogisticRegression()\n",
    "            # instantiate the One Vs Rest Classifier.\n",
    "            OVRclassifier = OneVsRest(classifier=lr)\n",
    "#             fitModel = OVRclassifier.fit(train)\n",
    "            # Add parameters of your choice here:\n",
    "            paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "                .build()\n",
    "            #Cross Validator requires the following parameters:\n",
    "            crossval = CrossValidator(estimator=OVRclassifier,\n",
    "                                      estimatorParamMaps=paramGrid,\n",
    "                                      evaluator=MulticlassClassificationEvaluator(),\n",
    "                                      numFolds=2) # 3 is best practice\n",
    "            # Run cross-validation, and choose the best set of parameters.\n",
    "            fitModel = crossval.fit(train)\n",
    "            return fitModel\n",
    "        if Mtype == \"MultilayerPerceptronClassifier\":\n",
    "            # specify layers for the neural network:\n",
    "            # input layer of size features, two intermediate of features+1 and same size as features\n",
    "            # and output of size number of classes\n",
    "            # Note: crossvalidator cannot be used here\n",
    "            features_count = len(features[0][0])\n",
    "            layers = [features_count, features_count+1, features_count, classes]\n",
    "            MPC_classifier = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "            fitModel = MPC_classifier.fit(train)\n",
    "            return fitModel\n",
    "        if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2: # These classifiers currently only accept binary classification\n",
    "            print(Mtype,\" could not be used because PySpark currently only accepts binary classification data for this algorithm\")\n",
    "            return\n",
    "        if Mtype in(\"LogisticRegression\",\"NaiveBayes\",\"RandomForestClassifier\",\"GBTClassifier\",\"LinearSVC\",\"DecisionTreeClassifier\"):\n",
    "  \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"LogisticRegression\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "#                              .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
    "                             .addGrid(classifier.maxIter, [10, 15,20])\n",
    "                             .build())\n",
    "                \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"NaiveBayes\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                             .addGrid(classifier.smoothing, [0.0, 0.2, 0.4, 0.6]) \\\n",
    "                             .build())\n",
    "                \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"RandomForestClassifier\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                               .addGrid(classifier.maxDepth, [2, 5, 10])\n",
    "#                                .addGrid(classifier.maxBins, [5, 10, 20])\n",
    "#                                .addGrid(classifier.numTrees, [5, 20, 50])\n",
    "                             .build())\n",
    "                \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"GBTClassifier\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "#                              .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "#                              .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
    "                             .addGrid(classifier.maxIter, [10, 15,50,100])\n",
    "                             .build())\n",
    "                \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"LinearSVC\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                             .addGrid(classifier.maxIter, [10, 15]) \\\n",
    "                             .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
    "                             .build())\n",
    "            \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"DecisionTreeClassifier\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "#                              .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "                             .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
    "                             .build())\n",
    "            \n",
    "            #Cross Validator requires all of the following parameters:\n",
    "            crossval = CrossValidator(estimator=classifier,\n",
    "                                      estimatorParamMaps=paramGrid,\n",
    "                                      evaluator=MulticlassClassificationEvaluator(),\n",
    "                                      numFolds=2) # 3 + is best practice\n",
    "            # Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "            fitModel = crossval.fit(train)\n",
    "            return fitModel\n",
    "    \n",
    "    fitModel = IntanceFitModel(Mtype,classifier,classes,features,train)\n",
    "    \n",
    "    # Print feature selection metrics\n",
    "    if fitModel is not None:\n",
    "        \n",
    "        if Mtype in(\"OneVsRest\"):\n",
    "            # Get Best Model\n",
    "            BestModel = fitModel.bestModel\n",
    "            print(\" \")\n",
    "            print('\\033[1m' + Mtype + '\\033[0m')\n",
    "            # Extract list of binary models\n",
    "            models = BestModel.models\n",
    "            for model in models:\n",
    "                print('\\033[1m' + 'Intercept: '+ '\\033[0m',model.intercept,'\\033[1m' + '\\nCoefficients:'+ '\\033[0m',model.coefficients)\n",
    "\n",
    "        if Mtype == \"MultilayerPerceptronClassifier\":\n",
    "            print(\"\")\n",
    "            print('\\033[1m' + Mtype,\" Weights\"+ '\\033[0m')\n",
    "            print('\\033[1m' + \"Model Weights: \"+ '\\033[0m',fitModel.weights.size)\n",
    "            print(\"\")\n",
    "\n",
    "        if Mtype in(\"DecisionTreeClassifier\", \"GBTClassifier\",\"RandomForestClassifier\"):\n",
    "            # FEATURE IMPORTANCES\n",
    "            # Estimate of the importance of each feature.\n",
    "            # Each featureâ€™s importance is the average of its importance across all trees \n",
    "            # in the ensemble The importance vector is normalized to sum to 1. \n",
    "            # Get Best Model\n",
    "            BestModel = fitModel.bestModel\n",
    "            print(\" \")\n",
    "            print('\\033[1m' + Mtype,\" Feature Importances\"+ '\\033[0m')\n",
    "            print(\"(Scores add up to 1)\")\n",
    "            print(\"Lowest score is the least important\")\n",
    "            print(\" \")\n",
    "            print(BestModel.featureImportances)\n",
    "            \n",
    "            if Mtype in(\"DecisionTreeClassifier\"):\n",
    "                global DT_featureimportances\n",
    "                DT_featureimportances = BestModel.featureImportances.toArray()\n",
    "                global DT_BestModel\n",
    "                DT_BestModel = BestModel\n",
    "            if Mtype in(\"GBTClassifier\"):\n",
    "                global GBT_featureimportances\n",
    "                GBT_featureimportances = BestModel.featureImportances.toArray()\n",
    "                global GBT_BestModel\n",
    "                GBT_BestModel = BestModel\n",
    "            if Mtype in(\"RandomForestClassifier\"):\n",
    "                global RF_featureimportances\n",
    "                RF_featureimportances = BestModel.featureImportances.toArray()\n",
    "                global RF_BestModel\n",
    "                RF_BestModel = BestModel\n",
    "\n",
    "        if Mtype in(\"LogisticRegression\"):\n",
    "            # Get Best Model\n",
    "            BestModel = fitModel.bestModel\n",
    "            print(\" \")\n",
    "            print('\\033[1m' + Mtype,\" Coefficient Matrix\"+ '\\033[0m')\n",
    "            print(\"You should compares these relative to eachother\")\n",
    "            print(\"Coefficients: \\n\" + str(BestModel.coefficientMatrix))\n",
    "            print(\"Intercept: \" + str(BestModel.interceptVector))\n",
    "            global LR_coefficients\n",
    "            LR_coefficients = BestModel.coefficientMatrix.toArray()\n",
    "            global LR_BestModel\n",
    "            LR_BestModel = BestModel\n",
    "\n",
    "        if Mtype in(\"LinearSVC\"):\n",
    "            # Get Best Model\n",
    "            BestModel = fitModel.bestModel\n",
    "            print(\" \")\n",
    "            print('\\033[1m' + Mtype,\" Coefficients\"+ '\\033[0m')\n",
    "            print(\"You should compares these relative to eachother\")\n",
    "            print(\"Coefficients: \\n\" + str(BestModel.coefficients))\n",
    "            global LSVC_coefficients\n",
    "            LSVC_coefficients = BestModel.coefficients.toArray()\n",
    "            global LSVC_BestModel\n",
    "            LSVC_BestModel = BestModel\n",
    "        \n",
    "   \n",
    "    # Set the column names to match the external results dataframe that we will join with later:\n",
    "    columns = ['Classifier', 'Result']\n",
    "    \n",
    "    if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2:\n",
    "        Mtype = [Mtype] # make this a list\n",
    "        score = [\"N/A\"]\n",
    "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
    "    else:\n",
    "        predictions = fitModel.transform(test)\n",
    "        MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\") # redictionCol=\"prediction\",\n",
    "        accuracy = (MC_evaluator.evaluate(predictions))*100\n",
    "        Mtype = [Mtype] # make this a string\n",
    "        score = [str(accuracy)] #make this a string and convert to a list\n",
    "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
    "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
    "        \n",
    "    return result\n",
    "    #Also returns the fit model important scores or p values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T10:08:28.435168Z",
     "start_time": "2020-08-19T10:08:28.292589Z"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.classification import *\n",
    "# from pyspark.ml.evaluation import *\n",
    "# from pyspark.sql import functions\n",
    "# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Comment out Naive Bayes if your data still contains negative values\n",
    "classifiers = [\n",
    "                LogisticRegression()\n",
    "                ,OneVsRest()\n",
    "               ,LinearSVC()\n",
    "               ,NaiveBayes()\n",
    "               ,RandomForestClassifier()\n",
    "               ,GBTClassifier()\n",
    "               ,DecisionTreeClassifier()\n",
    "               ,MultilayerPerceptronClassifier()\n",
    "              ] \n",
    "\n",
    "featureDF_list = [HTFfeaturizedData,TFIDFfeaturizedData,W2VfeaturizedData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T10:15:43.003225Z",
     "start_time": "2020-08-19T10:08:33.914782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mHTFfeaturizedData  Results:\u001b[0m\n",
      " \n",
      "\u001b[1mLogisticRegression  Coefficient Matrix\u001b[0m\n",
      "You should compares these relative to eachother\n",
      "Coefficients: \n",
      "DenseMatrix([[-0.11691609,  0.10138117,  0.19504664,  0.10715306,  0.12025501,\n",
      "               0.08651916,  0.13521009,  0.04111269,  0.13934382,  0.17979542,\n",
      "               0.16423726,  0.19798202,  0.06275585,  0.00541656,  0.20581733,\n",
      "              -0.07753202,  0.04680764, -0.04383274, -0.04489838,  0.25238338],\n",
      "             [ 0.14651825, -0.24506996,  0.02623206, -0.15583047, -0.00321263,\n",
      "              -0.06969515, -0.06923569,  0.07052051, -0.08566868, -0.06659006,\n",
      "              -0.07268842,  0.06184688,  0.08103145,  0.02106505, -0.02687312,\n",
      "              -0.07816375, -0.02432487,  0.19753572,  0.00080057,  0.00317896],\n",
      "             [ 0.06232509,  0.1554158 , -0.09452352,  0.10882901,  0.01683968,\n",
      "               0.09936838,  0.0400252 , -0.05018513, -0.03144719, -0.13456191,\n",
      "              -0.01436535, -0.11693092, -0.27718187, -0.20529332, -0.06270672,\n",
      "              -0.02075416, -0.07167844, -0.10066177, -0.29477989, -0.10690543],\n",
      "             [-0.09192724, -0.01172701, -0.12675518, -0.0601516 , -0.13388205,\n",
      "              -0.11619239, -0.1059996 , -0.06144808, -0.02222795,  0.02135655,\n",
      "              -0.07718349, -0.14289798,  0.13339456,  0.1788117 , -0.1162375 ,\n",
      "               0.17644993,  0.04919567, -0.05304121,  0.33887771, -0.14865691]])\n",
      "Intercept: [-0.3242207154879592,0.056886662109612005,0.3069595384375583,-0.03962548505921111]\n",
      " \n",
      "\u001b[1mOneVsRest\u001b[0m\n",
      "\u001b[1mIntercept: \u001b[0m -1.4828277856159544 \u001b[1m\n",
      "Coefficients:\u001b[0m [-0.16838498151495837,0.11185160207707255,0.24371279240053714,0.13068558135272718,0.1466000118432542,0.10259556839602654,0.16924445985421283,0.04989960746089719,0.18022523273781427,0.22818489560422123,0.20772704772959705,0.24667327144342463,0.06533068263656291,-0.002667622118438937,0.2597302114361745,-0.10710643694078727,0.05918451431536304,-0.07145726060060752,-0.09308797381981025,0.32125283799089205]\n",
      "\u001b[1mIntercept: \u001b[0m -1.0246678748228866 \u001b[1m\n",
      "Coefficients:\u001b[0m [0.1972735139893675,-0.2910548279868341,0.010145034265472357,-0.196960997673648,-0.021007580799838547,-0.09955017062901839,-0.0971677527922592,0.08646311802102541,-0.11565222665149479,-0.09159998521003632,-0.10233820532527343,0.054080465900953854,0.08132375116446071,0.01703712024277545,-0.05542861129495785,-0.08379768634771836,-0.03045740095261734,0.2532444420829687,-0.03360074599810941,-0.0380199022303519]\n",
      "\u001b[1mIntercept: \u001b[0m -0.6860372360449579 \u001b[1m\n",
      "Coefficients:\u001b[0m [0.07161070598356507,0.1758567192496699,-0.1431181208453028,0.1157397913901452,0.0005054433463718602,0.10777794355625936,0.020138796378200333,-0.06826339593900015,-0.059680588451913784,-0.19139734385448767,-0.04411893878709957,-0.18134200991723226,-0.3205101457004428,-0.24474461787446794,-0.11642210641351121,-0.023710214043861623,-0.08757580965006659,-0.12766938177581835,-0.33218597347796613,-0.17153785518056294]\n",
      "\u001b[1mIntercept: \u001b[0m -1.149450889736942 \u001b[1m\n",
      "Coefficients:\u001b[0m [-0.10042753191515774,-0.044803957995175,-0.17710393931803883,-0.08486965976479481,-0.16341292767054927,-0.14289881656449788,-0.12882053533106677,-0.08076936987055727,-0.03931381188986774,0.0013384120202957666,-0.10432212482063405,-0.1873774030258683,0.14002499912644778,0.21714908065450494,-0.1515762956754525,0.2283436230514698,0.05673258247055076,-0.0687650582647811,0.41710415157840297,-0.19875565905649828]\n",
      "LinearSVC  could not be used because PySpark currently only accepts binary classification data for this algorithm\n",
      " \n",
      "\u001b[1mRandomForestClassifier  Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "(20,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],[0.05985291329584534,0.05984394036801723,0.04667810898722498,0.044461151987755826,0.039472217846159285,0.04219370867920801,0.039074180033995286,0.037616328509485,0.03829690828666527,0.04142603184685556,0.04527608893619426,0.05158377818770258,0.05024555109117701,0.05447980269998234,0.04967125040066798,0.05381987203780251,0.03557993637313015,0.04891268949114852,0.10336174431706699,0.05815379662391603])\n",
      "GBTClassifier  could not be used because PySpark currently only accepts binary classification data for this algorithm\n",
      " \n",
      "\u001b[1mDecisionTreeClassifier  Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "(20,[0,1,2,5,6,11,12,14,17,18,19],[0.18107397559639657,0.13146749306664848,0.11936200524992976,0.0038990065511962228,0.009447822476186,0.04891720857901618,0.0813089143816968,0.010782972617630086,0.025284587675597197,0.3713881404478288,0.017067873357874006])\n",
      "\n",
      "\u001b[1mMultilayerPerceptronClassifier  Weights\u001b[0m\n",
      "\u001b[1mModel Weights: \u001b[0m 965\n",
      "\n",
      "+------------------------------+------+\n",
      "|Classifier                    |Result|\n",
      "+------------------------------+------+\n",
      "|LogisticRegression            |35.74 |\n",
      "|OneVsRest                     |35.63 |\n",
      "|LinearSVC                     |N/A   |\n",
      "|NaiveBayes                    |35.02 |\n",
      "|RandomForestClassifier        |35.77 |\n",
      "|GBTClassifier                 |N/A   |\n",
      "|DecisionTreeClassifier        |33.06 |\n",
      "|MultilayerPerceptronClassifier|36.16 |\n",
      "+------------------------------+------+\n",
      "\n",
      "None\n",
      "\u001b[1mTFIDFfeaturizedData  Results:\u001b[0m\n",
      " \n",
      "\u001b[1mLogisticRegression  Coefficient Matrix\u001b[0m\n",
      "You should compares these relative to eachother\n",
      "Coefficients: \n",
      "DenseMatrix([[-0.10441287,  0.07736225,  0.1460141 ,  0.08703464,  0.09387803,\n",
      "               0.07597086,  0.09797853,  0.03244485,  0.11329738,  0.13301275,\n",
      "               0.13309783,  0.14806496,  0.05099639,  0.0050399 ,  0.16458449,\n",
      "              -0.06910515,  0.03531473, -0.0353304 , -0.03796473,  0.19603072],\n",
      "             [ 0.13084931, -0.18700871,  0.01963761, -0.12657267, -0.00250797,\n",
      "              -0.06119801, -0.0501709 ,  0.05565258, -0.06965531, -0.04926336,\n",
      "              -0.05890667,  0.04625347,  0.06584743,  0.01960023, -0.02148943,\n",
      "              -0.06966822, -0.01835226,  0.15921925,  0.00067694,  0.00246916],\n",
      "             [ 0.05565993,  0.11859515, -0.07076137,  0.08839593,  0.01314603,\n",
      "               0.08725351,  0.02900383, -0.03960453, -0.02556901, -0.09954898,\n",
      "              -0.01164168, -0.08744922, -0.22524232, -0.19101757, -0.05014424,\n",
      "              -0.01849841, -0.05407888, -0.08113617, -0.24925706, -0.08303538],\n",
      "             [-0.08209637, -0.00894868, -0.09489035, -0.04885789, -0.1045161 ,\n",
      "              -0.10202636, -0.07681146, -0.0484929 , -0.01807306,  0.01579959,\n",
      "              -0.06254948, -0.10686922,  0.1083985 ,  0.16637744, -0.09295082,\n",
      "               0.15727178,  0.03711641, -0.04275268,  0.28654485, -0.1154645 ]])\n",
      "Intercept: [-0.3242207154879599,0.0568866621096121,0.3069595384375591,-0.03962548505921132]\n",
      " \n",
      "\u001b[1mOneVsRest\u001b[0m\n",
      "\u001b[1mIntercept: \u001b[0m -1.4828277856159546 \u001b[1m\n",
      "Coefficients:\u001b[0m [-0.15037758043794827,0.08535205099316417,0.1824461288813751,0.10614884011184153,0.11444447415993725,0.09008725077188216,0.12264116625484447,0.03937920946212957,0.14653714911345764,0.16881131151996714,0.16834194072497916,0.18447972127392326,0.053088734379827904,-0.0024821202173551994,0.20769662198998762,-0.09546515353283849,0.04465264634553175,-0.05759652918132801,-0.07871240617846456,0.24952287428011102]\n",
      "\u001b[1mIntercept: \u001b[0m -1.024667874822887 \u001b[1m\n",
      "Coefficients:\u001b[0m [0.17617671986724992,-0.2220989780997607,0.007594686396528519,-0.1599807816127724,-0.01639973631503184,-0.08741314392081842,-0.0704115605028973,0.06823398837289303,-0.09403426659344603,-0.06776571953884292,-0.08293485264949088,0.0404451978821713,0.06608495197209255,0.015852387902995365,-0.044324205736027085,-0.07468980596660267,-0.022979043911630977,0.20412202729076973,-0.028411785737458548,-0.02953074389535874]\n",
      "\u001b[1mIntercept: \u001b[0m -0.6860372360449581 \u001b[1m\n",
      "Coefficients:\u001b[0m [0.0639525247583034,0.1341932649167223,-0.10713982989489056,0.09400918206646547,0.00039457839918195017,0.09463779752504926,0.014593360851627697,-0.05387133695159197,-0.04852496599008166,-0.14159586046201242,-0.03575397551413165,-0.13562038257743217,-0.26045155667190095,-0.22772549377067647,-0.09309844277775595,-0.021133176386448793,-0.06607288582096113,-0.10290505417094752,-0.2808865226973272,-0.13323654645414526]\n",
      "\u001b[1mIntercept: \u001b[0m -1.1494508897369422 \u001b[1m\n",
      "Coefficients:\u001b[0m [-0.08968762607498156,-0.03418913664611363,-0.1325819946501377,-0.06893504127593333,-0.12756961164633382,-0.1254767795930422,-0.09334840682043256,-0.06374066042001074,-0.031965190598503994,0.000990157950209429,-0.08454262044738439,-0.1401340764687263,0.11378650406140489,0.20204890323373403,-0.12120994477873188,0.2035252003098976,0.04280274951362799,-0.05542653960862192,0.35269079399358694,-0.1543771057008413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC  could not be used because PySpark currently only accepts binary classification data for this algorithm\n",
      " \n",
      "\u001b[1mRandomForestClassifier  Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "(20,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],[0.05985291329584534,0.05984394036801723,0.04667810898722498,0.044461151987755826,0.039472217846159285,0.04219370867920801,0.039074180033995286,0.037616328509485,0.03829690828666527,0.04142603184685556,0.04527608893619426,0.05158377818770258,0.05024555109117701,0.05447980269998234,0.04967125040066798,0.05381987203780251,0.03557993637313015,0.04891268949114852,0.10336174431706699,0.05815379662391603])\n",
      "GBTClassifier  could not be used because PySpark currently only accepts binary classification data for this algorithm\n",
      " \n",
      "\u001b[1mDecisionTreeClassifier  Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "(20,[0,1,2,5,6,11,12,14,17,18,19],[0.18107397559639657,0.13146749306664848,0.11936200524992976,0.0038990065511962228,0.009447822476186,0.04891720857901618,0.0813089143816968,0.010782972617630086,0.025284587675597197,0.3713881404478288,0.017067873357874006])\n",
      "\n",
      "\u001b[1mMultilayerPerceptronClassifier  Weights\u001b[0m\n",
      "\u001b[1mModel Weights: \u001b[0m 965\n",
      "\n",
      "+------------------------------+------+\n",
      "|Classifier                    |Result|\n",
      "+------------------------------+------+\n",
      "|LogisticRegression            |35.74 |\n",
      "|OneVsRest                     |35.63 |\n",
      "|LinearSVC                     |N/A   |\n",
      "|NaiveBayes                    |34.99 |\n",
      "|RandomForestClassifier        |35.77 |\n",
      "|GBTClassifier                 |N/A   |\n",
      "|DecisionTreeClassifier        |33.06 |\n",
      "|MultilayerPerceptronClassifier|36.04 |\n",
      "+------------------------------+------+\n",
      "\n",
      "None\n",
      "\u001b[1mW2VfeaturizedData  Results:\u001b[0m\n",
      " \n",
      "\u001b[1mLogisticRegression  Coefficient Matrix\u001b[0m\n",
      "You should compares these relative to eachother\n",
      "Coefficients: \n",
      "DenseMatrix([[-0.56103618, -9.20859462, 11.95804707],\n",
      "             [-3.49736091, -0.43567413,  1.83855329],\n",
      "             [ 1.98861493,  5.09118147, -6.0138568 ],\n",
      "             [ 2.06978215,  4.55308728, -7.78274357]])\n",
      "Intercept: [-1.0110173718583406,0.6942139801276084,-0.2671838458760385,0.5839872376067707]\n",
      " \n",
      "\u001b[1mOneVsRest\u001b[0m\n",
      "\u001b[1mIntercept: \u001b[0m -10.424162111291631 \u001b[1m\n",
      "Coefficients:\u001b[0m [4.814660249380486,0.9729329465758858,15.909573386387255]\n",
      "\u001b[1mIntercept: \u001b[0m 5.721276323316617 \u001b[1m\n",
      "Coefficients:\u001b[0m [-9.6692595836692,-5.509208683126771,-2.203812507423371]\n",
      "\u001b[1mIntercept: \u001b[0m -2.448563903843004 \u001b[1m\n",
      "Coefficients:\u001b[0m [1.0687897013487557,6.265470330339222,-4.2165605824788]\n",
      "\u001b[1mIntercept: \u001b[0m 5.18246839491211 \u001b[1m\n",
      "Coefficients:\u001b[0m [-0.5915359740474331,-5.680620257525677,-8.907486507029668]\n",
      "LinearSVC  could not be used because PySpark currently only accepts binary classification data for this algorithm\n",
      " \n",
      "\u001b[1mRandomForestClassifier  Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "(3,[0,1,2],[0.31990162934516003,0.278964021227761,0.40113434942707904])\n",
      "GBTClassifier  could not be used because PySpark currently only accepts binary classification data for this algorithm\n",
      " \n",
      "\u001b[1mDecisionTreeClassifier  Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "(3,[0,1,2],[0.25727746026367054,0.21101863492192466,0.5317039048144048])\n",
      "\n",
      "\u001b[1mMultilayerPerceptronClassifier  Weights\u001b[0m\n",
      "\u001b[1mModel Weights: \u001b[0m 47\n",
      "\n",
      "+------------------------------+------+\n",
      "|Classifier                    |Result|\n",
      "+------------------------------+------+\n",
      "|LogisticRegression            |45.86 |\n",
      "|OneVsRest                     |53.18 |\n",
      "|LinearSVC                     |N/A   |\n",
      "|NaiveBayes                    |30.86 |\n",
      "|RandomForestClassifier        |59.96 |\n",
      "|GBTClassifier                 |N/A   |\n",
      "|DecisionTreeClassifier        |56.65 |\n",
      "|MultilayerPerceptronClassifier|53.35 |\n",
      "+------------------------------+------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for featureDF in featureDF_list:\n",
    "    print('\\033[1m' + featureDF.name,\" Results:\"+ '\\033[0m')\n",
    "    train, test = featureDF.randomSplit([0.7, 0.3],seed = 11)\n",
    "    features = featureDF.select(['features']).collect()\n",
    "    # Learn how many classes there are in order to specify evaluation type based on binary or multi and turn the df into an object\n",
    "    class_count = featureDF.select(countDistinct(\"label\")).collect()\n",
    "    classes = class_count[0][0]\n",
    "\n",
    "    #set up your results table\n",
    "    columns = ['Classifier', 'Result']\n",
    "    vals = [(\"Place Holder\",\"N/A\")]\n",
    "    results = spark.createDataFrame(vals, columns)\n",
    "\n",
    "    for classifier in classifiers:\n",
    "        new_result = ClassTrainEval(classifier,features,classes,train,test)\n",
    "        results = results.union(new_result)\n",
    "    results = results.where(\"Classifier!='Place Holder'\")\n",
    "    print(results.show(10,truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:16:12.186994Z",
     "start_time": "2020-08-19T07:16:12.069404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|               TITLE|CATEGORY|               words|            filtered|label|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|fed official says...|       b|[fed, official, s...|[fed, official, s...|  1.0|\n",
      "|feds charles plos...|       b|[feds, charles, p...|[feds, charles, p...|  1.0|\n",
      "|us open stocks fa...|       b|[us, open, stocks...|[us, open, stocks...|  1.0|\n",
      "|fed risks falling...|       b|[fed, risks, fall...|[fed, risks, fall...|  1.0|\n",
      "|feds plosser nast...|       b|[feds, plosser, n...|[feds, plosser, n...|  1.0|\n",
      "|plosser fed may h...|       b|[plosser, fed, ma...|[plosser, fed, ma...|  1.0|\n",
      "|feds plosser tape...|       b|[feds, plosser, t...|[feds, plosser, t...|  1.0|\n",
      "|feds plosser expe...|       b|[feds, plosser, e...|[feds, plosser, e...|  1.0|\n",
      "|us jobs growth la...|       b|[us, jobs, growth...|[us, jobs, growth...|  1.0|\n",
      "|ecb unlikely to e...|       b|[ecb, unlikely, t...|[ecb, unlikely, e...|  1.0|\n",
      "|ecb unlikely to e...|       b|[ecb, unlikely, t...|[ecb, unlikely, e...|  1.0|\n",
      "|eus halfbaked ban...|       b|[eus, halfbaked, ...|[eus, halfbaked, ...|  1.0|\n",
      "|europe reaches cr...|       b|[europe, reaches,...|[europe, reaches,...|  1.0|\n",
      "|ecb focusstronger...|       b|[ecb, focusstrong...|[ecb, focusstrong...|  1.0|\n",
      "|eu aims for deal ...|       b|[eu, aims, for, d...|[eu, aims, deal, ...|  1.0|\n",
      "|forex  pound drop...|       b|[forex, pound, dr...|[forex, pound, dr...|  1.0|\n",
      "|noyer says strong...|       b|[noyer, says, str...|[noyer, says, str...|  1.0|\n",
      "|eu week ahead mar...|       b|[eu, week, ahead,...|[eu, week, ahead,...|  1.0|\n",
      "|ecb member noyer ...|       b|[ecb, member, noy...|[ecb, member, noy...|  1.0|\n",
      "|euro anxieties wa...|       b|[euro, anxieties,...|[euro, anxieties,...|  1.0|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_data.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T09:33:10.711507Z",
     "start_time": "2020-08-19T09:33:02.798995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------+\n",
      "|prediction|            features|               TITLE|CATEGORY|\n",
      "+----------+--------------------+--------------------+--------+\n",
      "|       0.0|(26792,[93,285,17...| acm awards winne...|       e|\n",
      "|       1.0|(26792,[3,54,76,1...| and  year mortga...|       b|\n",
      "|       2.0|(26792,[664,4758,...| are finalists fo...|       m|\n",
      "|       2.0|(26792,[207,494,6...| are now finalist...|       m|\n",
      "|       0.0|(26792,[16,85,353...| arrests made dur...|       e|\n",
      "+----------+--------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Accuracy of NaiveBayes is = 0.90724\n",
      "Test Error of NaiveBayes = 0.0927603 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\")\n",
    "model = cv.fit(feature_data)\n",
    "countVectorizer_feateures = model.transform(feature_data)\n",
    "\n",
    "(trainingData, testData) = countVectorizer_feateures.randomSplit([0.7, 0.3],seed = 11)\n",
    "\n",
    "nb = NaiveBayes(modelType=\"multinomial\",labelCol=\"label\", featuresCol=\"features\")\n",
    "nbModel = nb.fit(trainingData)\n",
    "nb_predictions = nbModel.transform(testData)\n",
    "\n",
    "nb_predictions.select(\"prediction\", \"features\", 'TITLE', 'CATEGORY').show(5)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "nb_accuracy = evaluator.evaluate(nb_predictions)\n",
    "print(\"Accuracy of NaiveBayes is = %g\"% (nb_accuracy))\n",
    "print(\"Test Error of NaiveBayes = %g \" % (1.0 - nb_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
