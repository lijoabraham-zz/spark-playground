{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Classification using Apache Spark\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Based on the features in the dataset, we will be creating a model which will predict the patient has heart disease or not. We will be using various classification algorithms in Apache spark and select best alogirthm based on the prediction score.\n",
    "\n",
    "## About Dataset\n",
    "\n",
    "We have a dataset of details of patients who has heart disease or not based on the features in it.\n",
    "\n",
    "- age - age in years\n",
    "- sex - (1 = male; 0 = female)\n",
    "- cp - chest pain type\n",
    "- trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n",
    "- chol - serum cholestoral in mg/dl\n",
    "- fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n",
    "- restecg - resting electrocardiographic results\n",
    "- thalach - maximum heart rate achieved\n",
    "- exang - exercise induced angina (1 = yes; 0 = no)\n",
    "- oldpeak - ST depression induced by exercise relative to rest\n",
    "- slope - the slope of the peak exercise ST segment\n",
    "- ca - number of major vessels (0-3) colored by flourosopy\n",
    "- thal - 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "- target - have disease or not (1=yes, 0=no)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a spark Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:28:45.220650Z",
     "start_time": "2020-08-21T13:28:40.469863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.49:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>heart_disease</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc25d685198>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"heart_disease\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from csv and creating a Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:28:50.840592Z",
     "start_time": "2020-08-21T13:28:45.230212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "| 63|  1|  3|     145| 233|  1|      0|    150|    0|    2.3|    0|  0|   1|     1|\n",
      "| 37|  1|  2|     130| 250|  0|      1|    187|    0|    3.5|    0|  0|   2|     1|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv(\"./data/datasets_heart.csv\", inferSchema=True, header=True)\n",
    "df.show(2)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark needs all the feature columns to be numerical values and as the all columns in dataframe are integers , we are good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:28:50.890425Z",
     "start_time": "2020-08-21T13:28:50.843506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- cp: integer (nullable = true)\n",
      " |-- trestbps: integer (nullable = true)\n",
      " |-- chol: integer (nullable = true)\n",
      " |-- fbs: integer (nullable = true)\n",
      " |-- restecg: integer (nullable = true)\n",
      " |-- thalach: integer (nullable = true)\n",
      " |-- exang: integer (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- slope: integer (nullable = true)\n",
      " |-- ca: integer (nullable = true)\n",
      " |-- thal: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:28:52.947741Z",
     "start_time": "2020-08-21T13:28:50.892855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|target|count|\n",
      "+------+-----+\n",
      "|     1|  165|\n",
      "|     0|  138|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('target').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to see how much data in the dataframe that has null values. We can drop the data if the percentage of null values is very less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:28:55.258823Z",
     "start_time": "2020-08-21T13:28:52.949937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is clean with no null values\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "def null_value_calc(df):\n",
    "    null_columns_counts = []\n",
    "    numRows = df.count()\n",
    "    for k in df.columns:\n",
    "        nullRows = df.where(col(k).isNull()).count()\n",
    "        if(nullRows > 0):\n",
    "            temp = k,nullRows,(nullRows/numRows)*100\n",
    "            null_columns_counts.append(temp)\n",
    "    return(null_columns_counts)\n",
    "\n",
    "null_columns_calc_list = null_value_calc(df)\n",
    "if null_columns_calc_list : \n",
    "    spark.createDataFrame(null_columns_calc_list, ['Column_Name', 'Null_Values_Count','Null_Value_Percent']).show()\n",
    "else :\n",
    "    print(\"Data is clean with no null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:28:55.274233Z",
     "start_time": "2020-08-21T13:28:55.261130Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in dependencies\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, MinMaxScaler\n",
    "from pyspark.sql.types import * \n",
    "\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to prepare the DF before training the data. We can use spark built in function StringIndexer to convert all the strings to Integers.\n",
    "\n",
    "We can normalize the data to treat outliers using the built in skewness function. \n",
    "\n",
    "We can treat negative values by using MinMaxScaler, as some classification algorithms like Naive Bayes will not accept negative values. \n",
    "\n",
    "Lastly we can convert all the values to vectors by using VectorAssembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:28:55.395283Z",
     "start_time": "2020-08-21T13:28:55.277525Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Data Prep function\n",
    "def MLClassifierDFPrep(df,input_columns,dependent_var,treat_outliers=True,treat_neg_values=True):\n",
    "    \n",
    "    # change label (class variable) to string type to prep for reindexing\n",
    "    # Pyspark is expecting a zero indexed integer for the label column. \n",
    "    # Just incase our data is not in that format... we will treat it by using the StringIndexer built in method\n",
    "    renamed = df.withColumn(\"label_str\", df[dependent_var].cast(StringType())) #Rename and change to string type\n",
    "    indexer = StringIndexer(inputCol=\"label_str\", outputCol=\"label\") #Pyspark is expecting the this naming convention \n",
    "    indexed = indexer.fit(renamed).transform(renamed)\n",
    "    print(indexed.groupBy(dependent_var,\"label\").count().show(100))\n",
    "\n",
    "    # Convert all string type data in the input column list to numeric\n",
    "    # Otherwise the Algorithm will not be able to process it\n",
    "    numeric_inputs = []\n",
    "    string_inputs = []\n",
    "    for column in input_columns:\n",
    "        if str(indexed.schema[column].dataType) == 'StringType':\n",
    "            indexer = StringIndexer(inputCol=column, outputCol=column+\"_num\") \n",
    "            indexed = indexer.fit(indexed).transform(indexed)\n",
    "            new_col_name = column+\"_num\"\n",
    "            string_inputs.append(new_col_name)\n",
    "        else:\n",
    "            numeric_inputs.append(column)\n",
    "            \n",
    "    if treat_outliers == True:\n",
    "        print(\"We are correcting for non normality now!\")\n",
    "        # empty dictionary d\n",
    "        d = {}\n",
    "        # Create a dictionary of quantiles\n",
    "        for col in numeric_inputs: \n",
    "            d[col] = indexed.approxQuantile(col,[0.01,0.99],0.25) #if you want to make it go faster increase the last number\n",
    "        #Now fill in the values\n",
    "        for col in numeric_inputs:\n",
    "            skew = indexed.agg(skewness(indexed[col])).collect() #check for skewness\n",
    "            skew = skew[0][0]\n",
    "            # This function will floor, cap and then log+1 (just in case there are 0 values)\n",
    "            if skew > 1:\n",
    "                indexed = indexed.withColumn(col, \\\n",
    "                log(when(df[col] < d[col][0],d[col][0])\\\n",
    "                .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "                .otherwise(indexed[col] ) +1).alias(col))\n",
    "                print(col+\" has been treated for positive (right) skewness. (skew =)\",skew,\")\")\n",
    "            elif skew < -1:\n",
    "                indexed = indexed.withColumn(col, \\\n",
    "                exp(when(df[col] < d[col][0],d[col][0])\\\n",
    "                .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "                .otherwise(indexed[col] )).alias(col))\n",
    "                print(col+\" has been treated for negative (left) skewness. (skew =\",skew,\")\")\n",
    "\n",
    "            \n",
    "    # Produce a warning if there are negative values in the dataframe that Naive Bayes cannot be used. \n",
    "    # Note: we only need to check the numeric input values since anything that is indexed won't have negative values\n",
    "    minimums = df.select([min(c).alias(c) for c in df.columns if c in numeric_inputs]) # Calculate the mins for all columns in the df\n",
    "    min_array = minimums.select(array(numeric_inputs).alias(\"mins\")) # Create an array for all mins and select only the input cols\n",
    "    df_minimum = min_array.select(array_min(min_array.mins)).collect() # Collect golobal min as Python object\n",
    "    df_minimum = df_minimum[0][0] # Slice to get the number itself\n",
    "\n",
    "    features_list = numeric_inputs + string_inputs\n",
    "    assembler = VectorAssembler(inputCols=features_list,outputCol='features')\n",
    "    output = assembler.transform(indexed).select('features','label')\n",
    "\n",
    "#     final_data = output.select('features','label') #drop everything else\n",
    "    \n",
    "    # Now check for negative values and ask user if they want to correct that? \n",
    "    if df_minimum < 0:\n",
    "        print(\" \")\n",
    "        print(\"WARNING: The Naive Bayes Classifier will not be able to process your dataframe as it contains negative values\")\n",
    "        print(\" \")\n",
    "    \n",
    "    if treat_neg_values == True:\n",
    "        print(\"You have opted to correct that by rescaling all your features to a range of 0 to 1\")\n",
    "        print(\" \")\n",
    "        print(\"We are rescaling you dataframe....\")\n",
    "        scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "        # Compute summary statistics and generate MinMaxScalerModel\n",
    "        scalerModel = scaler.fit(output)\n",
    "\n",
    "        # rescale each feature to range [min, max].\n",
    "        scaled_data = scalerModel.transform(output)\n",
    "        final_data = scaled_data.select('label','scaledFeatures') # added class to the selection\n",
    "        final_data = final_data.withColumnRenamed('scaledFeatures','features')\n",
    "        print(\"Done!\")\n",
    "\n",
    "    else:\n",
    "        print(\"You have opted not to correct that therefore you will not be able to use to Naive Bayes classifier\")\n",
    "        print(\"We will return the dataframe unscaled.\")\n",
    "        final_data = output\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to train and evaluate all the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:28:55.539314Z",
     "start_time": "2020-08-21T13:28:55.403133Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def ClassTrainEval(classifier,features,classes,folds,train,test):\n",
    "    \n",
    "    def FindMtype(classifier):\n",
    "        # Intstantiate Model\n",
    "        M = classifier\n",
    "        # Learn what it is\n",
    "        Mtype = type(M).__name__\n",
    "        \n",
    "        return Mtype\n",
    "    \n",
    "    Mtype = FindMtype(classifier)\n",
    "    \n",
    "\n",
    "    def IntanceFitModel(Mtype,classifier,classes,features,folds,train):\n",
    "        \n",
    "        if Mtype == \"OneVsRest\":\n",
    "            # instantiate the base classifier.\n",
    "            lr = LogisticRegression()\n",
    "            # instantiate the One Vs Rest Classifier.\n",
    "            OVRclassifier = OneVsRest(classifier=lr)\n",
    "#             fitModel = OVRclassifier.fit(train)\n",
    "            # Add parameters of your choice here:\n",
    "            paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "                .build()\n",
    "            #Cross Validator requires the following parameters:\n",
    "            crossval = CrossValidator(estimator=OVRclassifier,\n",
    "                                      estimatorParamMaps=paramGrid,\n",
    "                                      evaluator=MulticlassClassificationEvaluator(),\n",
    "                                      numFolds=folds) # 3 is best practice\n",
    "            # Run cross-validation, and choose the best set of parameters.\n",
    "            fitModel = crossval.fit(train)\n",
    "            return fitModel\n",
    "        if Mtype == \"MultilayerPerceptronClassifier\":\n",
    "            # specify layers for the neural network:\n",
    "            # input layer of size features, two intermediate of features+1 and same size as features\n",
    "            # and output of size number of classes\n",
    "            # Note: crossvalidator cannot be used here\n",
    "            features_count = len(features[0][0])\n",
    "            layers = [features_count, features_count+1, features_count, classes]\n",
    "            MPC_classifier = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "            fitModel = MPC_classifier.fit(train)\n",
    "            return fitModel\n",
    "        if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2: # These classifiers currently only accept binary classification\n",
    "            print(Mtype,\" could not be used because PySpark currently only accepts binary classification data for this algorithm\")\n",
    "            return\n",
    "        if Mtype in(\"LogisticRegression\",\"NaiveBayes\",\"RandomForestClassifier\",\"GBTClassifier\",\"LinearSVC\",\"DecisionTreeClassifier\"):\n",
    "  \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"LogisticRegression\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "#                              .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
    "                             .addGrid(classifier.maxIter, [10, 15,20])\n",
    "                             .build())\n",
    "                \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"NaiveBayes\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                             .addGrid(classifier.smoothing, [0.0, 0.2, 0.4, 0.6]) \\\n",
    "                             .build())\n",
    "                \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"RandomForestClassifier\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                               .addGrid(classifier.maxDepth, [2, 5, 10])\n",
    "#                                .addGrid(classifier.maxBins, [5, 10, 20])\n",
    "#                                .addGrid(classifier.numTrees, [5, 20, 50])\n",
    "                             .build())\n",
    "                \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"GBTClassifier\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "#                              .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "#                              .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
    "                             .addGrid(classifier.maxIter, [10, 15,50,100])\n",
    "                             .build())\n",
    "                \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"LinearSVC\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                             .addGrid(classifier.maxIter, [10, 15]) \\\n",
    "                             .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
    "                             .build())\n",
    "            \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"DecisionTreeClassifier\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "#                              .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "                             .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
    "                             .build())\n",
    "            \n",
    "            #Cross Validator requires all of the following parameters:\n",
    "            crossval = CrossValidator(estimator=classifier,\n",
    "                                      estimatorParamMaps=paramGrid,\n",
    "                                      evaluator=MulticlassClassificationEvaluator(),\n",
    "                                      numFolds=folds) # 3 + is best practice\n",
    "            # Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "            fitModel = crossval.fit(train)\n",
    "            return fitModel\n",
    "    \n",
    "    fitModel = IntanceFitModel(Mtype,classifier,classes,features,folds,train)\n",
    "    \n",
    "    # Print feature selection metrics\n",
    "    if fitModel is not None:\n",
    "        \n",
    "        if Mtype in(\"OneVsRest\"):\n",
    "            # Get Best Model\n",
    "            BestModel = fitModel.bestModel\n",
    "            global OVR_BestModel\n",
    "            OVR_BestModel = BestModel\n",
    "            print(\" \")\n",
    "            print('\\033[1m' + Mtype + '\\033[0m')\n",
    "            # Extract list of binary models\n",
    "            models = BestModel.models\n",
    "            for model in models:\n",
    "                print('\\033[1m' + 'Intercept: '+ '\\033[0m',model.intercept)\n",
    "                print('\\033[1m' + 'Top 20 Coefficients:'+ '\\033[0m')\n",
    "                coeff_array = model.coefficients.toArray()\n",
    "                coeff_scores = []\n",
    "                for x in coeff_array:\n",
    "                    coeff_scores.append(float(x))\n",
    "                # Then zip with input_columns list and create a df\n",
    "                result = spark.createDataFrame(zip(input_columns,coeff_scores), schema=['feature','coeff'])\n",
    "                print(result.orderBy(result[\"coeff\"].desc()).show(truncate=False))\n",
    "\n",
    "\n",
    "        if Mtype == \"MultilayerPerceptronClassifier\":\n",
    "            print(\"\")\n",
    "            print('\\033[1m' + Mtype + '\\033[0m')\n",
    "            print('\\033[1m' + \"Model Weights: \"+ '\\033[0m',fitModel.weights.size)\n",
    "            print(\"\")\n",
    "            global MLPC_Model\n",
    "            MLPC_BestModel = fitModel\n",
    "\n",
    "        if Mtype in(\"DecisionTreeClassifier\", \"GBTClassifier\",\"RandomForestClassifier\"):\n",
    "            # FEATURE IMPORTANCES\n",
    "            # Estimate of the importance of each feature.\n",
    "            # Each featureâ€™s importance is the average of its importance across all trees \n",
    "            # in the ensemble The importance vector is normalized to sum to 1. \n",
    "            # Get Best Model\n",
    "            BestModel = fitModel.bestModel\n",
    "            print(\" \")\n",
    "            print('\\033[1m' + Mtype,\" Top 20 Feature Importances\"+ '\\033[0m')\n",
    "            print(\"(Scores add up to 1)\")\n",
    "            print(\"Lowest score is the least important\")\n",
    "            print(\" \")\n",
    "            featureImportances = BestModel.featureImportances.toArray()\n",
    "            # Convert from numpy array to list\n",
    "            imp_scores = []\n",
    "            for x in featureImportances:\n",
    "                imp_scores.append(float(x))\n",
    "            # Then zip with input_columns list and create a df\n",
    "            result = spark.createDataFrame(zip(input_columns,imp_scores), schema=['feature','score'])\n",
    "            print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
    "            \n",
    "            # Save the feature importance values and the models\n",
    "            if Mtype in(\"DecisionTreeClassifier\"):\n",
    "                global DT_featureimportances\n",
    "                DT_featureimportances = BestModel.featureImportances.toArray()\n",
    "                global DT_BestModel\n",
    "                DT_BestModel = BestModel\n",
    "            if Mtype in(\"GBTClassifier\"):\n",
    "                global GBT_featureimportances\n",
    "                GBT_featureimportances = BestModel.featureImportances.toArray()\n",
    "                global GBT_BestModel\n",
    "                GBT_BestModel = BestModel\n",
    "            if Mtype in(\"RandomForestClassifier\"):\n",
    "                global RF_featureimportances\n",
    "                RF_featureimportances = BestModel.featureImportances.toArray()\n",
    "                global RF_BestModel\n",
    "                RF_BestModel = BestModel\n",
    "\n",
    "        # Print the coefficients\n",
    "        if Mtype in(\"LogisticRegression\"):\n",
    "            # Get Best Model\n",
    "            BestModel = fitModel.bestModel\n",
    "            print(\" \")\n",
    "            print('\\033[1m' + Mtype + '\\033[0m')\n",
    "            print(\"Intercept: \" + str(BestModel.interceptVector))\n",
    "            print('\\033[1m' + \" Top 20 Coefficients\"+ '\\033[0m')\n",
    "            print(\"You should compares these relative to eachother\")\n",
    "            # Convert from numpy array to list\n",
    "            coeff_array = BestModel.coefficientMatrix.toArray()\n",
    "            coeff_scores = []\n",
    "            for x in coeff_array[0]:\n",
    "                coeff_scores.append(float(x))\n",
    "            # Then zip with input_columns list and create a df\n",
    "            result = spark.createDataFrame(zip(input_columns,coeff_scores), schema=['feature','coeff'])\n",
    "            print(result.orderBy(result[\"coeff\"].desc()).show(truncate=False))\n",
    "            # Save the coefficient values and the models\n",
    "            global LR_coefficients\n",
    "            LR_coefficients = BestModel.coefficientMatrix.toArray()\n",
    "            global LR_BestModel\n",
    "            LR_BestModel = BestModel\n",
    "\n",
    "        # Print the Coefficients\n",
    "        if Mtype in(\"LinearSVC\"):\n",
    "            # Get Best Model\n",
    "            BestModel = fitModel.bestModel\n",
    "            print(\" \")\n",
    "            print('\\033[1m' + Mtype + '\\033[0m')\n",
    "            print(\"Intercept: \" + str(BestModel.intercept))\n",
    "            print('\\033[1m' + \"Top 20 Coefficients\"+ '\\033[0m')\n",
    "            print(\"You should compares these relative to eachother\")\n",
    "#             print(\"Coefficients: \\n\" + str(BestModel.coefficients))\n",
    "            coeff_array = BestModel.coefficients.toArray()\n",
    "            coeff_scores = []\n",
    "            for x in coeff_array:\n",
    "                coeff_scores.append(float(x))\n",
    "            # Then zip with input_columns list and create a df\n",
    "            result = spark.createDataFrame(zip(input_columns,coeff_scores), schema=['feature','coeff'])\n",
    "            print(result.orderBy(result[\"coeff\"].desc()).show(truncate=False))\n",
    "            # Save the coefficient values and the models\n",
    "            global LSVC_coefficients\n",
    "            LSVC_coefficients = BestModel.coefficients.toArray()\n",
    "            global LSVC_BestModel\n",
    "            LSVC_BestModel = BestModel\n",
    "        \n",
    "   \n",
    "    # Set the column names to match the external results dataframe that we will join with later:\n",
    "    columns = ['Classifier', 'Result']\n",
    "    \n",
    "    if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2:\n",
    "        Mtype = [Mtype] # make this a list\n",
    "        score = [\"N/A\"]\n",
    "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
    "    else:\n",
    "        predictions = fitModel.transform(test)\n",
    "        MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\") # redictionCol=\"prediction\",\n",
    "        accuracy = (MC_evaluator.evaluate(predictions))*100\n",
    "        Mtype = [Mtype] # make this a string\n",
    "        score = [str(accuracy)] #make this a string and convert to a list\n",
    "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
    "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
    "        \n",
    "    return result\n",
    "    #Also returns the fit model important scores or p values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data for evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:28:56.883215Z",
     "start_time": "2020-08-21T13:28:55.541560Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up independ and dependent vars\n",
    "input_columns = df.columns\n",
    "input_columns = input_columns[:-1] # keep only relevant columns: everything but the first and last cols\n",
    "dependent_var = 'target'\n",
    "\n",
    "# Learn how many classes there are in order to specify evaluation type based on binary or multi and turn the df into an object\n",
    "class_count = df.select(countDistinct(\"target\")).collect()\n",
    "classes = class_count[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:28:56.892282Z",
     "start_time": "2020-08-21T13:28:56.885236Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../lib\")\n",
    "from sparkml import SparkMl as myMl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:28:57.017722Z",
     "start_time": "2020-08-21T13:28:56.895061Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sparkml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-192aa6336833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sparkml' is not defined"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(sparkml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:41:55.276981Z",
     "start_time": "2020-08-21T13:38:55.395329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|target|label|count|\n",
      "+------+-----+-----+\n",
      "|     1|  0.0|  165|\n",
      "|     0|  1.0|  138|\n",
      "+------+-----+-----+\n",
      "\n",
      "None\n",
      "We are correcting for non normality now!\n",
      "chol has been treated for positive (right) skewness. (skew =) 1.1377326187082237 )\n",
      "fbs has been treated for positive (right) skewness. (skew =) 1.9768034646834516 )\n",
      "oldpeak has been treated for positive (right) skewness. (skew =) 1.2634255245891595 )\n",
      "ca has been treated for positive (right) skewness. (skew =) 1.303925955673585 )\n",
      "You have opted to correct that by rescaling all your features to a range of 0 to 1\n",
      " \n",
      "We are rescaling you dataframe....\n",
      "Done!\n",
      " \n",
      "\u001b[1mLogisticRegression\u001b[0m\n",
      "Intercept: [-0.27858395831265814]\n",
      "\u001b[1m Top 20 Coefficients\u001b[0m\n",
      "You should compares these relative to eachother\n",
      "+--------+--------------------+\n",
      "|feature |coeff               |\n",
      "+--------+--------------------+\n",
      "|ca      |3.2561403119742103  |\n",
      "|thal    |1.9507972678985093  |\n",
      "|oldpeak |1.823041688290639   |\n",
      "|sex     |1.5159468059497392  |\n",
      "|exang   |0.9025106275727564  |\n",
      "|trestbps|0.6343948233277691  |\n",
      "|chol    |0.4634029661825217  |\n",
      "|age     |-0.11431029435627435|\n",
      "|fbs     |-0.24043765743716938|\n",
      "|restecg |-1.0511838461232212 |\n",
      "|slope   |-1.389309442040661  |\n",
      "|cp      |-2.6264781753904236 |\n",
      "|thalach |-3.6914083541278107 |\n",
      "+--------+--------------------+\n",
      "\n",
      "None\n",
      " \n",
      "\u001b[1mOneVsRest\u001b[0m\n",
      "\u001b[1mIntercept: \u001b[0m 2.813358861813215\n",
      "\u001b[1mTop 20 Coefficients:\u001b[0m\n",
      "+--------+-------------------+\n",
      "|feature |coeff              |\n",
      "+--------+-------------------+\n",
      "|thalach |2.3547707473773967 |\n",
      "|cp      |2.3029938769599863 |\n",
      "|slope   |1.0992891869201753 |\n",
      "|restecg |0.5499980360172287 |\n",
      "|fbs     |0.14132129654156206|\n",
      "|age     |-0.7102586862340479|\n",
      "|exang   |-0.8371558061658709|\n",
      "|trestbps|-0.9173611390560056|\n",
      "|sex     |-1.5113975470410543|\n",
      "|chol    |-1.8045717197148823|\n",
      "|oldpeak |-2.0649332383184205|\n",
      "|thal    |-2.1860501441943603|\n",
      "|ca      |-2.8004346466011203|\n",
      "+--------+-------------------+\n",
      "\n",
      "None\n",
      "\u001b[1mIntercept: \u001b[0m -2.813358861814543\n",
      "\u001b[1mTop 20 Coefficients:\u001b[0m\n",
      "+--------+--------------------+\n",
      "|feature |coeff               |\n",
      "+--------+--------------------+\n",
      "|ca      |2.8004346466026395  |\n",
      "|thal    |2.1860501441918054  |\n",
      "|oldpeak |2.0649332383064425  |\n",
      "|chol    |1.8045717197095568  |\n",
      "|sex     |1.5113975470384462  |\n",
      "|trestbps|0.9173611390586623  |\n",
      "|exang   |0.8371558061702324  |\n",
      "|age     |0.7102586862508866  |\n",
      "|fbs     |-0.14132129654788553|\n",
      "|restecg |-0.549998036016811  |\n",
      "|slope   |-1.0992891869315153 |\n",
      "|cp      |-2.3029938769619003 |\n",
      "|thalach |-2.354770747368169  |\n",
      "+--------+--------------------+\n",
      "\n",
      "None\n",
      " \n",
      "\u001b[1mLinearSVC\u001b[0m\n",
      "Intercept: -0.01143347875801648\n",
      "\u001b[1mTop 20 Coefficients\u001b[0m\n",
      "You should compares these relative to eachother\n",
      "+--------+---------------------+\n",
      "|feature |coeff                |\n",
      "+--------+---------------------+\n",
      "|ca      |1.2327442207793953   |\n",
      "|oldpeak |0.9778183104391681   |\n",
      "|thal    |0.7965379715787552   |\n",
      "|exang   |0.6247955684845177   |\n",
      "|age     |0.46966706653206924  |\n",
      "|sex     |0.3470097647869571   |\n",
      "|trestbps|0.10906062261348654  |\n",
      "|chol    |0.0                  |\n",
      "|fbs     |-0.024473855486809193|\n",
      "|restecg |-0.35391240678126207 |\n",
      "|slope   |-0.8701434471211159  |\n",
      "|cp      |-1.176185673927346   |\n",
      "|thalach |-1.8534052773741558  |\n",
      "+--------+---------------------+\n",
      "\n",
      "None\n",
      " \n",
      "\u001b[1mRandomForestClassifier  Top 20 Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "+--------+--------------------+\n",
      "|feature |score               |\n",
      "+--------+--------------------+\n",
      "|ca      |0.24747466644974225 |\n",
      "|thal    |0.23376641222453093 |\n",
      "|exang   |0.15669787852814776 |\n",
      "|cp      |0.12163692632900427 |\n",
      "|oldpeak |0.10023942814937441 |\n",
      "|thalach |0.09195331598380536 |\n",
      "|age     |0.026822120385644087|\n",
      "|slope   |0.0171413223620456  |\n",
      "|trestbps|0.004267929587705318|\n",
      "|restecg |0.0                 |\n",
      "|sex     |0.0                 |\n",
      "|fbs     |0.0                 |\n",
      "|chol    |0.0                 |\n",
      "+--------+--------------------+\n",
      "\n",
      "None\n",
      " \n",
      "\u001b[1mGBTClassifier  Top 20 Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "+--------+--------------------+\n",
      "|feature |score               |\n",
      "+--------+--------------------+\n",
      "|age     |0.1702894909413673  |\n",
      "|cp      |0.16797722247388472 |\n",
      "|thal    |0.15495055223199528 |\n",
      "|oldpeak |0.12355244024241412 |\n",
      "|ca      |0.10220665851747011 |\n",
      "|trestbps|0.06517664189665932 |\n",
      "|thalach |0.060910963367947775|\n",
      "|chol    |0.05155913635174183 |\n",
      "|sex     |0.04573453611475528 |\n",
      "|slope   |0.03175431090752087 |\n",
      "|exang   |0.013972384600705509|\n",
      "|fbs     |0.008574781988088923|\n",
      "|restecg |0.003340880365448981|\n",
      "+--------+--------------------+\n",
      "\n",
      "None\n",
      " \n",
      "\u001b[1mDecisionTreeClassifier  Top 20 Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "+--------+--------------------+\n",
      "|feature |score               |\n",
      "+--------+--------------------+\n",
      "|cp      |0.3029198732564282  |\n",
      "|ca      |0.2164220072608827  |\n",
      "|age     |0.13618236697569205 |\n",
      "|thal    |0.0942351642866672  |\n",
      "|sex     |0.07169775814244299 |\n",
      "|oldpeak |0.06666873085366232 |\n",
      "|chol    |0.06524125951516842 |\n",
      "|thalach |0.025249510958127552|\n",
      "|trestbps|0.021383328750928497|\n",
      "|fbs     |0.0                 |\n",
      "|slope   |0.0                 |\n",
      "|exang   |0.0                 |\n",
      "|restecg |0.0                 |\n",
      "+--------+--------------------+\n",
      "\n",
      "None\n",
      "\n",
      "\u001b[1mMultilayerPerceptronClassifier\u001b[0m\n",
      "\u001b[1mModel Weights: \u001b[0m 419\n",
      "\n",
      "!!!!!Final Results!!!!!!!!\n",
      "+------------------------------+------+\n",
      "|Classifier                    |Result|\n",
      "+------------------------------+------+\n",
      "|LogisticRegression            |85.24 |\n",
      "|OneVsRest                     |88.52 |\n",
      "|LinearSVC                     |80.32 |\n",
      "|NaiveBayes                    |75.40 |\n",
      "|RandomForestClassifier        |81.96 |\n",
      "|GBTClassifier                 |72.13 |\n",
      "|DecisionTreeClassifier        |73.77 |\n",
      "|MultilayerPerceptronClassifier|80.32 |\n",
      "+------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call on data prep, train and evaluate functions\n",
    "test1_data = MLClassifierDFPrep(df,input_columns,dependent_var)\n",
    "test1_data.limit(5).toPandas()\n",
    "\n",
    "# Comment out Naive Bayes if your data still contains negative values\n",
    "classifiers = [\n",
    "                LogisticRegression()\n",
    "                ,OneVsRest()\n",
    "               ,LinearSVC()\n",
    "               ,NaiveBayes()\n",
    "               ,RandomForestClassifier()\n",
    "               ,GBTClassifier()\n",
    "               ,DecisionTreeClassifier()\n",
    "               ,MultilayerPerceptronClassifier()\n",
    "              ] \n",
    "\n",
    "train,test = test1_data.randomSplit([0.8,0.2])\n",
    "features = test1_data.select(['features']).collect()\n",
    "folds = 3 # because we have limited data\n",
    "\n",
    "#set up your results table\n",
    "columns = ['Classifier', 'Result']\n",
    "vals = [(\"Place Holder\",\"N/A\")]\n",
    "results = spark.createDataFrame(vals, columns)\n",
    "\n",
    "for classifier in classifiers:\n",
    "    new_result = ClassTrainEval(classifier,features,classes,folds,train,test)\n",
    "    results = results.union(new_result)\n",
    "results = results.where(\"Classifier!='Place Holder'\")\n",
    "print(\"!!!!!Final Results!!!!!!!!\")\n",
    "results.show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After seeing all the classification algorithm, we can see that LogisticRegression is performing well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:43:12.921723Z",
     "start_time": "2020-08-21T13:43:12.822804Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = LR_BestModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:43:23.818129Z",
     "start_time": "2020-08-21T13:43:23.635322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|[0.33333333333333...|[-2.0326178151165...|[0.11582057192750...|       1.0|\n",
      "|  0.0|[0.60416666666666...|[-2.8906998027597...|[0.05261522454003...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.filter(\"prediction==1\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:43:29.446379Z",
     "start_time": "2020-08-21T13:43:27.501567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|   34|\n",
      "|  1.0|   27|\n",
      "+-----+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|   35|\n",
      "|       1.0|   26|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.groupBy(\"label\").count().show()\n",
    "predictions.groupBy(\"prediction\").count().show()\n",
    "\n",
    "predictions.filter(\"prediction != label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T13:44:37.704168Z",
     "start_time": "2020-08-21T13:44:30.538480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\u001b[1mLogisticRegression\u001b[0m\n",
      "Intercept: [-0.39541824536789105]\n",
      "\u001b[1m Top 20 Coefficients\u001b[0m\n",
      "You should compares these relative to eachother\n",
      "+--------+--------------------+\n",
      "|feature |coeff               |\n",
      "+--------+--------------------+\n",
      "|thal    |3.1461336013013934  |\n",
      "|ca      |3.0323686016546962  |\n",
      "|trestbps|2.0677796930310386  |\n",
      "|oldpeak |1.914423233206576   |\n",
      "|sex     |1.5602527092507847  |\n",
      "|chol    |1.3479592889483896  |\n",
      "|exang   |0.7912272147575776  |\n",
      "|fbs     |-0.10671003010448048|\n",
      "|age     |-1.2963686758895452 |\n",
      "|slope   |-1.6240505574272122 |\n",
      "|restecg |-1.6874844816732404 |\n",
      "|cp      |-2.864908329773626  |\n",
      "|thalach |-4.85795399810548   |\n",
      "+--------+--------------------+\n",
      "\n",
      "None\n",
      "+------------------+------+\n",
      "|Classifier        |Result|\n",
      "+------------------+------+\n",
      "|LogisticRegression|84.61 |\n",
      "+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "classifiers = [ LogisticRegression()\n",
    "                ] \n",
    "\n",
    "#Select the top n features and view results\n",
    "n = 99\n",
    "\n",
    "# For Logistic regression or One vs Rest\n",
    "selector = ChiSqSelector(numTopFeatures=n, featuresCol=\"features\",\n",
    "                     outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
    "bestFeaturesDf = selector.fit(test1_data).transform(test1_data)\n",
    "bestFeaturesDf = bestFeaturesDf.select(\"label\",\"selectedFeatures\")\n",
    "bestFeaturesDf = bestFeaturesDf.withColumnRenamed(\"selectedFeatures\",\"features\")\n",
    "\n",
    "# Collect features\n",
    "features = bestFeaturesDf.select(['features']).collect()\n",
    "\n",
    "# Split\n",
    "train,test = bestFeaturesDf.randomSplit([0.8,0.2])\n",
    "\n",
    "# Specify folds\n",
    "folds = 3\n",
    "\n",
    "#set up your results table\n",
    "columns = ['Classifier', 'Result']\n",
    "vals = [(\"Place Holder\",\"N/A\")]\n",
    "results = spark.createDataFrame(vals, columns)\n",
    "\n",
    "for classifier in classifiers:\n",
    "    new_result = ClassTrainEval(classifier,features,classes,folds,train,test)\n",
    "    results = results.union(new_result)\n",
    "results = results.where(\"Classifier!='Place Holder'\")\n",
    "results.show(100,False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
